% Modified
\documentclass{l3proj}
\usepackage{wrapfig}
\usepackage{csquotes}
\SetBlockEnvironment{quotation}
\begin{document}
\title{Multi-device Recording System}
\author{Alastair Weir \\
        Gordon Adam \\
        Peter Yordanov \\
        Keir Smith \\
        Georgi Dimitrov}
\date{28 March 2014}
\maketitle
\begin{abstract}

Surrounding us in city life there are unqiue moments that are missed by viewing it through the lense of a a smartphone instead of being \'in the moment\' and realising how great the experience could be. This data captured is then restricted to that specific user, letting them only see it from their perspective. By creating a mobile first approach to capturing locational audio centred around these moments, MDRS can crowdsource new insights and perspectives of events in our cities. This aim was successfully achieved with a functioning web and mobile application to allow users to gather this information while still enjoying the moment. Our testing has shown...

\end{abstract}
\educationalconsent
\tableofcontents
%==============================================================================
\chapter{Introduction}
\label{intro}

%==============================================================================
\chapter{Research}
\label{Research}

Research in here will come in handy

%==============================================================================
\chapter{Planning}
\label{Planning}

Plans in here will come in handy


%==============================================================================
\chapter{Design}
\label{design}

The core of our design sprung from early storyboarding and imagined user
profiles which informed us extensively to what a possible system might look
like. Each team member chose a different scenario that they thought MDRS could be used and created a visual aid to describe the user's experience as they interacted with it As previously discussed, MDRS was viewed as a tool to democratise access to moments through an interactive medium. Storyboards served as an excellent tool to realise the potential of the project and its uses in such varied settings as public safety, political protests, music concerts and disaster areas.

\begin{figure}[ht!]
  \centering
\includegraphics[width=0.75\textwidth]{images/ally-storyboard.jpg}
\caption{An initial storyboard for MDRS.}
\end{figure}

The project's use in public safety was of particular interest to the team. MDRS could serve as an equaliser offering a non-invasive and non-threatening way for police officers to monitor their patrol. It could hold the officers and those they interact with accountable in a alternative way to current methods such as mounted video cameras.

Include another storyboard here {\bf bold}

From our storyboards we realised that MDRS should be more than an audio recorder with location tracking. By allowing the user to capture images, we could combine these three sources of information into a virtual environment that other users could explore.

\section{User Interface} The realisation that the project would be spread across
both mobile and static devices raised the need for two interaction models for
MDRS was found quickly. For capturing the various forms of data a mobile
implementation was key. Android was quickly chosen as the frontrunner for its
proliferation in the market, an expected ease of development due to its Java
base, which much of the team had experience in, and access to test devices
amongst the team. A web application counterpart was planned for the flexibility
of web technologies which allowed a wide variety of devices and users to access
this facet of the project rather than a native application for any given
Operating System would.

\subsection{Web Application}   Given the nature of MDRS, the team look to the
Distributed Information Management course taught in second semester as a source
of possible tools. Taught by Leif Azzopardi, the course is structured around the
use of Django, a Python based web framework which would offer us the flexibility
to create a rich, dynamic application easily deployable and widely available to
possible users. Widely used, Django had a wide range of support materials
available including Tango with Django written by Azzopardi and Glasgow
University postgraduate David Maxwell. This resource and knowing we would have
to learn it later through coursework made the framework choice an easy one.

\begin{figure}[ht!]
  \centering
\includegraphics[width=0.75\textwidth]{images/web-map-view.jpg}
\caption{Prototype of map view.}
\end{figure}

The key interaction between the user and MDRS is location therefore it made
sense to make a map the central interface. The user would be able to explore the
map and click on pins representing recordings. This would then project the path
of the recording, indicating where images were captured along the way. Users
could then play it back individually or select multiple recordings for
synchronous playback. With such an important role, the choice of which mapping
API to leverage was extensive in the early stages of the project.

Initially the team were attracted to OpenStreetMap with the very lightweight
Javascript library Leaflet. This would give us unlimited access, as the maps
were open source, but would require us to host the map tiles which amounted to
multiple gigabytes depending on the level of detail chosen. Other disadvantages
were the additional difficulty in initial configuration and setup. Another
option was Microsoft’s Bing maps which didn’t require as much configuration but
were more limited in their free level of access. As an alternative Google Maps
was chosen for its simple API and reasonable access for free users. Ultimately
is offered 100,000 map loads a month for free. We estimated for a typical user
spending 15 minutes on the website that 4 map loads would be needed making this
a workable figure for at least 25,000 user visits a month.

Alongside our use of maps, early on in development we prototyped a version which would incorporate Google's Street View. This would allow the user to follow a recording's location trail even more closely. By harvesting the orientation of the recording device we could show where they were looking, showing the Street View at that angle, and overlay any of the user's own images they captured on top of this. While this immersed the user even deeper into the recording, it was unfortunately not taken beyond this prototype stage due to the complexity of implementation.

\begin{figure}[ht!]
  \centering
\includegraphics[width=0.5\textwidth]{images/timeline-example.png}
\caption{An example of TimelineJS by KnightLab.}
\end{figure}

A second key component to this interface is the interactive timeline. This was
hoped to show the length of each recording and where they overlapped. Upon
clicking on one a dialog would show its metadata such as name, description, the
user who captured it and accompanying images. Early prototypes also show the
ability to move the current position of playback along the timeline to skip to
different positions. This level of manipulation was simplified upon selection of
TimelineJS which we used to construct it.

Early web interface designs featured a prominent header bar and a floating menu
panel which floated over the map. From early prototypes and wireframes this was
realised to waste a lot of screen real estate, detracting from the reason the
user was on the website. The header bar in particular became wasteful as the
ability to search all recordings was downgraded in importance within the
application. Through discussion in the team it was decided that users would more
likely be interested in audio based on location (the key paradigm of MDRS)
rather than searching through keyword. It gave us a stretch goal to aim for if
development moved along smoothly. Upon research into interface frameworks the
CSS library, PureCSS, was found as a replacement to create an engaging and
simple menu interface. Its simplicity favoured our team’s limited prior
experience with web design and it’s accompanying documentation is excellent.

\begin{figure}[ht!]
  \centering
\includegraphics[width=0.5\textwidth]{images/web-recording-view.jpg}
\caption{Recording specific prototype for web application.}
\end{figure}

To accompany this main map view of MDRS was a user page which offered a
personalised insight into a user’s recordings and activity with the application.
It included a personalised timeline and map showing their personal contributions
to the map from which they could play, edit details or delete them from the
service. Along with all the other tools mentioned, the foundations of our
web application’s interface were laid.

\subsection{Android Application} It was decided early on to approach this
element of the project realistically and define a simple set of requirements to
make the application simpler to implement and make the user experience focused
on the core of MDRS, capturing information from the world around us. This meant
it was purely a capture and upload system and it wouldn’t be an alternative to
the desktop viewing interface for interaction with other user’s recordings. Our
focus made the application more achievable while leaving scope for extension in
future development for these playback features.

\begin{wrapfigure}{r}{0.35\textwidth}
  \begin{center}
    \includegraphics[width=0.34\textwidth]{images/android-digital-prototype-1.jpg}
  \end{center}
  \caption{Initial interface design}
\end{wrapfigure}

In developing an engaging user interface on Android devices, the team looked for
a scalable and intuitive design drawing influence from a wide range of
applications such as Foursquare, Snapchat and Google Play’s suite of
applications while consistently referring to Google’s Android Design Guidelines.
Foursquare was looked to for its integration of a map into a larger whole while
Snapchat’s unconventional, yet intuitive navigation model was seen as a strength
of the application while making an attractive UI. Google’s applications
portrayed the strength and beauty of clear typography and the undeniable rule of
mobile design where less is often more. A utilitarian minimalism through design
was the goal. The core functionality of the application was single use and the
flow through it was clear allowing for a relatively simple approach to be taken,
finding place for flair within the constructs the team placed on themselves.

INSERT INITIAL DESIGN PROTOTYPE HERE

This first UI design offers a split with both a map and view through the camera
lens. The influence for the central record button came from Foursquare’s
prominent placement of their check-in button, placing the key functionality
centrally to draw attention. Downsides to this were its limitations across
smaller screens and a confusing interaction model. Could a user interact and
capture an image before hitting record? This lack of clarity(dw) lead to a
revisionary second prototype.

INSERT SECOND DESIGN PROTOTYPE HERE

This prototype drew more influence from Snapchat’s sliding interface, replacing
its horizontal movement with a vertical one. Again showing the record button on
the divide between a map and camera view, once the record was initiated the map
would slide up bringing up the camera view which would reveal from behind a
frosted glass effect the user’s view into the world they were recording,
prompting them to capture images. This was found very visually appealing and
ended up mostly being used in the final application.


%==============================================================================
\chapter{Implementation}
\label{impl}

In this chapter, we describe how the implemented the system.

%------------------------------------------------------------------------------
\section{Web Application}

Blah blah blah
Blah blah blah
Blah blah blah
Blah blah blah

% - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
\subsection{Server and Django configuration}

Blah blah blah
Blah blah blah
Blah blah blah
Blah blah blah

%------------------------------------------------------------------------------
\section{Android Application}

\subsection{Early development} With little experience in developing for Android,
our aim was to take the most key requirements and create simple demos of each
aspect. Then taking all the basic knowledge gained from this process, create an
application with the must have features, structured in a way to allow further development if time constraints allowed.

\subsubsection{Learning Resources and Tools} Google's Android tutorials served
as an early touchstone in development, offering an understanding of an
application's structure and layout in development. These go into great depth
about most of the classes and APIs useful to MDRS but often this depth was made
them inpenetrable. For example, you can either implement an interface for an
Activity though your Activity's .java file programmatically or in a separate
layout .xml file. Google's tutorials would switch between these methods,
muddying the Android developer's understanding of how to work in this new
development environment while adhering to good code standards.

Other learning resources were scarce and when found usually weren't to a good
enough standard to be useful. This meant that alongside Google's tutorials,
Stack Overflow was the main resource used to aid development. This crowdsourced
Q&A service served invaluable in fixing bugs and explaining topics. The answers
far exceeded the quality found in some textbooks that were used. Twitter also served as a direct communication to the development community with advice and encouragement coming from prominent developers.

As development began, an early decision made was to askew the use of Google's
new Android Studio to use Eclipse with the ADT (Android Development Tools)
plugin. This reduced the number of new tools to learn and avoided using unstable
software while trying to learn a new SDK. All of the development was done on
personal devices due to the University's lab machines not having the required
software and refusal from technicians to have it installed in a useful manner.
This limited development time as it had to be carried out almost exclusively at
home in off-hours. It also limited the chance of gaining any help from other
student's insights to solve common issues in Android development. Overall poor
access to tools caused extensive issues in the protracted  development cycle.

A Nexus 5 and Nexus 7 (2012 model) were used as testing devices. As Google
designed models, the Nexus range are a good standard for development as they run
a clean install of Android on common hardware shared across the device
ecosystem. This reduced the amount of testing required and avoided
troubleshooting vendor specific errors or bugs. These devices also allowed the
UI/UX could be tested on varying screen sizes to see how it reacted. As testing
expanded, the application was also later tested on an HTC One and a Samsung Galaxy
Note.

\subsubsection{Audio Recording}    Audio recording is key to MDRS' premise so
was the first thing to be implemented. Through our learning resources the
MediaRecorder class was chosen to implement this functionality. The alternative
to this was AudioRecord. This class captures the raw bytes of audio data which
allows for much more control over processing and format choices but at a higher
degree of difficulty. External Java libraries would be required alongside a more
complicated implementation making MediaRecorder the simpler choice while still
meeting our needs.

Prototypes were rapidly built referencing various tutorials. However these
prototypes repeatedly ran into complex issues and consumed an extensive portion
of development time available. MediaRecorder would consistently crash the test
devices with little debug information given by the error logs. Testing was
further slowed down as MediaRecorder doesn't work in the Android Virtual Machine
availble. This meant all testing was done on devices, complicating the entire
process. Combined with poor understanding of the Android SDK these tests caused
many issues and took up all Android development time between Mid-November and
late December 2013.

Eventually through extensive research surrounding MediaRecorder and learning
more about debugging Android code, the main problem was found to be in
interacting with external storage and the dichotomy between truly external
storage, such as a removeable SD Card, and internal-but-external storage. As
vendors moved to add internal storage in their Android devices, the OS would
still recognise that as an external source compared to ROM on the chipset. This
means that while it is internal, the OS views it as external. By creating a
/textit{Catch-22}, it made it difficult to access the correct storage in which
to place MDRS' data. With this problem found, the issue was resolved and
development continued.

\subsubsection{Google Play Services}    The next thing to integrate was the
Google Play Services library. This gives Android developers extensive access to
many of Google's services. For MDRS, the interest was soley in the Maps and Location APIs. Integrating this library into the project rasied multiple initial
issues. While adding a library should be a relatively simple task, there were
wildly varying instructions on how to do this, leading to a lot of confusion.
While it is possible to link it to the build path by manually downloading the
library and then linking it using Eclipse, other instruction sets described the
use of Apache's Ant or Maven files to decalre the dependencies. In practice
however these didn't work reliably. In the end it was decided to store the
library locally in the project's repository. This led to an issue with Eclipse.
Eclipse stores the full path to the library's folder on the computer statically
instead of just its path inside the current workspace directory. This meant when
switching between development environments (Windows and OSX) that it had to be
manually updated. Once these dependency problems were resolved development
quickly continued to the implementation of Maps.

For MDRS we required two map views, one as the main point of interest on the start screen and a second in the upload screen indicating the path their recording took with a start and end pin. These relatively simple requirements meant a lot of the code required could be reused from Google's example applications. The code required was relatively short but by using Play Services there was a lot of boilerplate code needed to check that it was installed on a device and to connect to Google's servers. A series of small tweaks were made to the default map, removing certain UI elements and changing the zoom levels. This manifested the team's first experience of inconsistency between devices. When running on a Nexus 5, the \'Show my location\' button, despite being explicitly declared, didn't appear while the same build of MDRS running on an HTC One did show the button.

The main issue that was found with the implementation of Maps was a bug which crashed the app if it was opened before another application using Google Maps had been opened on the device. While puzzling and initially demonstrating an erratic manifestation pattern, the problem was found to be referencing the device's last location, using getLastLocation(), when it didn't have one passing a null value and crashing the app.

Placing the map into an activity's layout was done by referring to a map fragment element which was then initialised programmatically. This displayed the benefits of Android's flexibility. Where initially having the option to programmatically do the layout or use XML was confusing, in this case it allowed the map to be placed in XML then modified in code. This made populating the map with pins and other markers a simple call of a method and feeding in data from the Location API.

While Android has its own Location classes, the Play Services' API was chosen for its higher level abstraction and excellent learning resources. While Android only had the reference documents, Play Services' was accompanied by tutorials and getting started guides. This sped up development while saving the team member working on it a lot of research for good learning resources.

\subsection{Main Development}
With these prototypes and test applications complete and the advent of the new year, the previous codebase was scrapped in early January. This fresh slate streamlined any previously written code and allowed a rethink of the structure from previous plans. While the application was originally going to be a single activity which would dynamically change, research showed this to be completely wrong. An activity as defined in Android is:

\blockquote{An Activity is an application component that provides a screen with which users can interact in order to do something, such as dial the phone, take a photo, send an email, or view a map. Each activity is given a window in which to draw its user interface. The window typically fills the screen, but may be smaller than the screen and float on top of other windows.}

Taking this into consideration the app was broken into three key stages, a MapView, recording and upload activity. This separation of concerns made the code more readable and easily understood. As development continued and more external classes were used and while not activitys were included in the \'src\' directory it could be seen that, while three activitys was better than one, a lot of the code required for the three main views the user had of the app could be separated into their own \'.java\' files to make the code much easier to understand but by this point there was not enough development time to refactor the entire application.








MAPS IMPL HERE!!

Using Google's excellent documentation for their APIs and the various example
applications on their developer's portal, the initial implementation of the maps
was relatively simple. Making use of the LocationService abstract data type and
using LocationManager allowed the application to gather information from the
device's GPS module or the next most accurate source of positional data. This is
then stored in a LinkedHashMap. Using this data structure allowed the Location
objects collected from LocationService to be stored in a chronological order
with their timestamp acting as each object's key. Parsing this information into
JSON later in the upload process was trivial due to this structured approach at
the data capture stage.

\subsection{Application Structure}
At this stage in development, the basis of what would form the application had been tested and was found to work roughly as expected. One aspect of development which had been unplanned was the structure and exact methods required for the Android application. These first few features were all in one main activity. An activity in Android is defined as (INSERT A BLOCK QUOTE FROM HERE:
http://developer.android.com/guide/components/activities.html).

REMEMBER TO TALK ABOUT STRUCTURE. HOW I TRIED TO MAKE IT ONE BIG ACTIVITY AND
THEN BROKE IT INTO THREE SUPPORTED BY MULTIPLE CLASSES. IN FUTURE WOULD CHANGE
THIS FURTHER TO BREAK ALL THE ACTIVITIES INTO SUB-ACTIVITIES EFFECTIVELY. BREAK
FUNCTIONALITY OUT INTO THEIR OWN CLASSES.

Google's Android tutorials served as an early touchstone in development,
offering an understanding of an application's structure and layout in
development. An early decision made was to askew the plan to use Google's
Android Studio to use Eclipse with the ADT (Android Development Tools) plugin.

%==============================================================================
\chapter{Evaluation}

We evaluated the project by...

%==============================================================================
\chapter{Challenges}
\label{Challenges}

A number of the challenges we encountered were through a lack of previous
experience and inaccurate expectations held by each member of the team. The
development process offered a lot of flexibility in finding weaknesses in our
ideas and restructuring them forge better working practices and develop new
insights into the workings of a small scale agile team.

\subsection{Communicational}
Our main communication issues was the over-complication and over dependence on multiple services for specific channels of communication. This diffused each team member's attention, turning a lot of communication into asking where other information was kept instead of working on the project.

As previously described we embarked on the project with the intention of using a Redmine instance to keep track of project details, GitHub as our VCS and Facebook for communication. This convoluted mix made it near impossible to keep track of discussion about specific issues and problems.

Our success with Facebook as a means of communication was varied. Its IM service was excellent allowing the team to communicate about the project as a group or one-to-one. Facebook's mobile applications made this easy to access while in transit which was an added benefit for those who commuted.

The Team's group page however was wholly unsucessful in our desired use case. The main point of weakness was the lack of chronological discourse of our messages. This meant you might have to scroll past a dozen posts to get yesterday's discussion just because it wasn't the most commented on post. The page's bloat UI also made it difficult to navigate with no search function to quickly find an old post quickly. While its feature which tells you who in the group has read the post was useful, the rest of the functionality was subpar.

Through other project work and happenstance, the team discovered GitHub's issue
tracker and wiki features which are individual to each repository. These quickly
became the default means of tracking progress for MDRS, replacing Redmine. The
customisable labels and ability to cross reference issues from commits were
invaluable. GitHub's fast and responsive web interface scaled well across
devices and meant everyone was able to be involved in decisions and contribute
issues to work on. Most of all it succeeded due to the team members being on GitHub to check commits of various projects and to collaborate on other projects. Whereas Redmine required an intent to visit it and keep up to date, GitHub just merged this information into an efficient user experience that had become a daily destination for the team during their work.

Surprisingly email notifications became a great source of information. As
default, GitHub sends out emails for every comment or new issue created. While
filling up inboxes, this device agnostic communication platform made for great
commute reading. While notifications can easily be lost in a endless-scroll, the
emails were small, actionable pieces of key information to keep track of the
project's direction as a whole.

We also found by replacing web-based interaction with face to face communication improved our productivity immensely. Even a quick 2 minute conversation could convey the same information a 20 minute IM chat could. While each team member kept their own schedule, we always took advantage of these conversations to keep one another up to date.

In future, due to the distributed nature of the team and constantly shifting
focus of attention for different deadlines, the team would leverage email more.
Weekly status reports would serve as talking points to hopefully make meetings
more productive, giving members time to prepare their thoughts. These would also
serve as evidence of communication and would be easily accessible at a later
date in a chronological order. While the agile principles can definitely be applied to a distributed workforce, using the methodology when the team cannot focus on one project for an extended period of time, in our usage, does not work. Switching between various projects and deadlines meant agile was reduced to agily moving between disciplines to complete the most important task to create a better end product.

\subsection{Technological}
Most technological issues revolved around a lack of experience and knowledge at
the beginning of the project. Issues such as handling dependencies with the package management system \textit{pip} with, initially, no requirements.txt file made setting up a new virtual environment a challenge. Everything had to be manually installed allowing for greater risk for failure and variation between team member's development environment. Our ignorance regarding virutal environments also made maintaining consistency harder throughout development. After working on Django development using /textit{Tango with Django}, the team studied and began to use virtual environments to simplify development. Until resolved, these issues caused a lot of problems for certain team members.

Another major misunderstanding came with the team using our version control system, git. If used correctly, git allows each developer to work on their local machine, testing and committing any changes to the repository. In our case this should have then been pulled on the server, effectively \'deploying\' our changes. Initially we were all using SSH to access our server and then working directly in the terminal, creating conflicts and locking file issues. This was quickly resolved when the team researched the issue, quickly changing to the correct workflow.

\subsection{Organisational}    At the beginning of the development cycle, agile roles were assigned for scrum master, communicator with supervisor, librarian and developer. Throughout development these roles merged and adhered to the agile principles even further. Due to the ad-hoc nature of the development team and shifting focus amongst projects, a different team member shifted into the scrum master role. This team member handled all secterial jobs such as keeping minutes at supervisor meetings and assigned each team member tasks in a bid to keep to the work schedules. These tasks were assigned taking into consideration each team member’s strengths and interests. This increased productivity within the team, pushing our development further and allowing us to expand on the feature set included in the final project.

%==============================================================================
\chapter{Future Work}
\label{Future Work}

Future work in here will come in handy

%==============================================================================
\chapter{Conclusion}

A great project!

%==============================================================================
\section{Contributions}

Conclusion here

%==============================================================================
\bibliographystyle{plain}
\bibliography{example}
\end{document}
